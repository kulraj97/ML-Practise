# -*- coding: utf-8 -*-
"""lr_multi_variate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xb6wvRmpKXOOgSil2wXl8Cco3lFt6d51
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from google.colab import drive

drive.mount('/content/gdrive')

dt=pd.read_csv("/content/gdrive/MyDrive/weatherAUS.csv")

dt.head()

#REMOVING THE OBJECTS WHICH ARE OF NO USES
dt.drop(['Date', 'Location','Humidity9am', 'WindGustDir','Pressure9am', 'WindDir9am','WindGustSpeed', 'WindDir3pm'], axis=1, inplace=True)
dt.head()

#FILLING THE MEAN VALUE IN LACE OF NAN
dt.fillna(dt.mean(),inplace=True)
dt.head()

#CONVERTING IT INTO BINARY DIGIT FOR LOGISTIC REGRESSION
dt.RainToday = [1 if each == 'Yes' else 0 for each in dt.RainToday]
dt.RainTomorrow = [1 if each == 'Yes' else 0 for each in dt.RainTomorrow]
dt.sample(3)

#SELECTING X & Y VALUES
x=dt.drop("RainTomorrow",axis=1)
y=dt["RainTomorrow"]

#mean normalisation for bringing all data b/w 0 and 1
x=(x-np.mean(x))/(np.max(x)-np.min(x))
y=(y-np.mean(y))/(np.max(y)-np.min(y))
x.head()

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

from sklearn.linear_model import LogisticRegression
lr=LogisticRegression()

y_test.shape



x_train=x_train.T

#APLLYING CONCEPT OF LOGISTIC REGRESSION
w=np.full([14,1],0.01)
w
b=0

y_train=y_train.values.reshape(-1,1)
y_train.shape

x_train.shape

def forward_backward_proagation(w, b, x_train, y_train):
  #forward proagation
  epochs=100
  learning_rate=1
  for i in range(epochs):
    cost=[]
    z=sum(np.dot(w.T,x_train)+b)
    y_head= y_head=1/1+np.exp(-z)
    N=len(y)
    loss=(-1/N)*np.sum((y_train*np.log(y_head)+(1-y_train)*np.log(1-y_head)))     #loss function formula 
    #backward roagation
    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1]
    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]
    gradients = {'derivative_weight': derivative_weight, 'derivative_bias': derivative_bias}
    w = w - learning_rate * gradients*derivative_weight
    b = b - learning_rate * gradients*derivative_bias
    cost.append(loss)
    return cost,w,b

print(forward_backward_proagation(w, b, x_train, y_train))

w.T.reshape(-1,1).shape

a=np.full([14,1],0.01)
s=sum(a)
s

